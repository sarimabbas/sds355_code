{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo code for Principal Components Analysis (PCA)\n",
    "\n",
    "We'll use this notebook to demonstrate PCA on some toy data and image data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gzip\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy data examples\n",
    "\n",
    "This is the code used to generate some of the first examples in the lecture slides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 500\n",
    "X = np.matrix(np.random.normal(size=2*n))\n",
    "X = X.reshape(n, 2)\n",
    "scale = .3\n",
    "X[:,1] = scale * X[:,1]\n",
    "theta = np.pi/4\n",
    "R = np.matrix([np.cos(theta), np.sin(theta), -np.sin(theta), np.cos(theta)]).reshape(2,2)\n",
    "X = X * R\n",
    "X[:,0] = X[:,0] - np.mean(X[:,0])\n",
    "X[:,1] = X[:,1] - np.mean(X[:,1])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(np.array(X[:,0]), np.array(X[:,1]))\n",
    "lim = np.max(np.abs(X))+.1\n",
    "plt.xlim(-lim,lim)\n",
    "plt.ylim(-lim,lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X)\n",
    "principal_vectors = pca.components_\n",
    "principal_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pc1 = principal_vectors[0]\n",
    "slope = pc1[1]/pc1[0]\n",
    "plt.figure()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(np.array(X[:,0]), np.array(X[:,1]))\n",
    "plt.plot([lim, -lim], [lim*slope, -lim*slope], 'k', color = 'r', linewidth=4)\n",
    "plt.xlim(-lim, lim)\n",
    "plt.ylim(-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check that the principal vectors are orthogonal\n",
    "np.dot(principal_vectors[0], principal_vectors[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST and Fashion MNIST\n",
    "\n",
    "Next we'll run PCA on the MNIST and Fashion MNIST data.\n",
    "\n",
    "To run the code, put the data in directories named `mnist` and `fashion-mnist` within the same directory as this notebook. \n",
    "\n",
    "You can download the data here:\n",
    "\n",
    "MNIST   http://yann.lecun.com/exdb/mnist/<br>\n",
    "FASHION-MNIST    https://github.com/zalandoresearch/fashion-mnist/tree/master/data/fashion\n",
    "\n",
    "Download the following files:<br>\n",
    "train-images-idx3-ubyte.gz<br>\n",
    "train-labels-idx1-ubyte.gz<br>\n",
    "t10k-images-idx3-ubyte.gz<br>\n",
    "t10k-labels-idx1-ubyte.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that is used to read mnist and fashion mnist data\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    data_dir = os.path.join(\"./\", dataset_name)\n",
    "        \n",
    "    def extract_data(filename, num_data, head_size, data_size):\n",
    "        with gzip.open(filename) as bytestream:\n",
    "            bytestream.read(head_size)\n",
    "            buf = bytestream.read(data_size * num_data)\n",
    "            data = np.frombuffer(buf, dtype=np.uint8).astype(np.float)\n",
    "        return data\n",
    "\n",
    "    data = extract_data(data_dir + '/train-images-idx3-ubyte.gz', 60000, 16, 28 * 28)\n",
    "    trX = data.reshape((60000, 28, 28))\n",
    "\n",
    "    data = extract_data(data_dir + '/train-labels-idx1-ubyte.gz', 60000, 8, 1)\n",
    "    trY = data.reshape((60000))\n",
    "\n",
    "    data = extract_data(data_dir + '/t10k-images-idx3-ubyte.gz', 10000, 16, 28 * 28)\n",
    "    teX = data.reshape((10000, 28, 28))\n",
    "\n",
    "    data = extract_data(data_dir + '/t10k-labels-idx1-ubyte.gz', 10000, 8, 1)\n",
    "    teY = data.reshape((10000))\n",
    "\n",
    "    trY = np.asarray(trY)\n",
    "    teY = np.asarray(teY)\n",
    "\n",
    "    X = np.concatenate((trX, teX), axis=0)\n",
    "    y = np.concatenate((trY, teY), axis=0).astype(np.int)\n",
    "\n",
    "    seed = 409\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(y)\n",
    "    return X / 255., y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, titles, h, w, n_row=3, n_col=4, reversed=False):\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        if reversed:\n",
    "            plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        else:\n",
    "            plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray.reversed())\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "x, y = load_data('mnist')\n",
    "height, width = (28, 28)\n",
    "X = x.reshape([70000, height*width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = X[(y==3), :]\n",
    "avgimg = images.mean(0)\n",
    "_ = plt.imshow(avgimg.reshape((28, 28)), cmap=plt.cm.gray.reversed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimages = np.subtract(images, avgimg)\n",
    "_ = plt.imshow(cimages[0].reshape((28, 28)), cmap=plt.cm.gray.reversed())\n",
    "_ = plt.imshow(np.add(cimages[0], avgimg).reshape((28, 28)), cmap=plt.cm.gray.reversed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_components = 25\n",
    "pca = PCA(num_components).fit(cimages)\n",
    "principal_vectors = pca.components_\n",
    "principal_vectors = principal_vectors.reshape((num_components, height, width))\n",
    "pcs = pca.fit_transform(cimages)\n",
    "capprox = pca.inverse_transform(pcs)\n",
    "labels = ['principal vector %d' % (i+1) for i in np.arange(num_components)]\n",
    "plot_images(principal_vectors, labels, height, width, int(num_components/5.), 5)\n",
    "ratio = pca.explained_variance_ratio_.sum()\n",
    "print('Variance explained by first %d principal vectors: %.2f%%' % (num_components, ratio*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx = np.add(capprox, avgimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "sample = np.random.choice(images.shape[0], num_samples)\n",
    "plot_images(images[sample,:], ['original' for i in range(num_samples)], height, width, 1, num_samples)\n",
    "plot_images(approx[sample,:], ['approx (%d)' % num_components for i in range(5)], height, width, 1, num_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "x, y = load_data('fashion-mnist')\n",
    "height, width = (28, 28)\n",
    "X = x.reshape([70000, height*width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = X[(y==3), :]\n",
    "avgimg = images.mean(0)\n",
    "_ = plt.imshow(avgimg.reshape((28, 28)), cmap=plt.cm.gray.reversed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimages = np.subtract(images, avgimg)\n",
    "_ = plt.imshow(cimages[1].reshape((28, 28)), cmap=plt.cm.gray.reversed())\n",
    "_ = plt.imshow(np.add(cimages[1], avgimg).reshape((28, 28)), cmap=plt.cm.gray.reversed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 100\n",
    "pca = PCA(num_components).fit(cimages)\n",
    "principal_vectors = pca.components_.reshape((num_components, height, width))\n",
    "pcs = pca.fit_transform(cimages)\n",
    "capprox = pca.inverse_transform(pcs)\n",
    "labels = ['principal vector %d' % (i+1) for i in np.arange(num_components)]\n",
    "plot_images(principal_vectors, labels, height, width, int(num_components/5.), 5)\n",
    "ratio = pca.explained_variance_ratio_.sum()\n",
    "print('Variance explained by first %d principal vectors: %.2f%%' % (num_components, ratio*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx = np.add(capprox, avgimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "sample = np.random.choice(images.shape[0], num_samples)\n",
    "plot_images(images[sample,:], ['original' for i in range(num_samples)], height, width, 1, num_samples)\n",
    "plot_images(approx[sample,:], ['approx (%d)' % num_components for i in range(5)], height, width, 1, num_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**labels**:\n",
    "0\tT-shirt/top\n",
    "1\tTrouser\n",
    "2\tPullover\n",
    "3\tDress\n",
    "4\tCoat\n",
    "5\tSandal\n",
    "6\tShirt\n",
    "7\tSneaker\n",
    "8\tBag\n",
    "9\tAnkle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Data\n",
    "\n",
    "Finally, we'll use the [\"Labeled Faces in the Wild\"](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html) dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "X = lfw_people.data\n",
    "y = lfw_people.target\n",
    "height, width = (50, 37)\n",
    "lfw_people.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label = 3\n",
    "images = X[(y==label),:]\n",
    "avgimg = images.mean(0)\n",
    "_ = plt.imshow(avgimg.reshape((height, width)), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimages = np.subtract(images, avgimg)\n",
    "_ = plt.imshow(cimages[2].reshape((height, width)), cmap=plt.cm.gray)\n",
    "_ = plt.imshow(np.add(cimages[2], avgimg).reshape((height, width)), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 50\n",
    "pca = PCA(num_components).fit(cimages)\n",
    "principal_vectors = pca.components_.reshape((num_components, height, width))\n",
    "pcs = pca.fit_transform(cimages)\n",
    "capprox = pca.inverse_transform(pcs)\n",
    "labels = ['principal vector %d' % (i+1) for i in np.arange(num_components)]\n",
    "plot_images(principal_vectors, labels, height, width, int(num_components/5.), 5, reversed=True)\n",
    "ratio = pca.explained_variance_ratio_.sum()\n",
    "print('%s' % lfw_people.target_names[label])\n",
    "print('Variance explained by first %d principal vectors: %.2f%%' % (num_components, ratio*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx = np.add(capprox, avgimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "sample = np.random.choice(images.shape[0], num_samples)\n",
    "plot_images(images[sample,:], ['original' for i in range(num_samples)], height, width, 1, num_samples, reversed=True)\n",
    "plot_images(approx[sample,:], ['approx (%d)' % num_components for i in range(5)], height, width, 1, num_samples, reversed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
