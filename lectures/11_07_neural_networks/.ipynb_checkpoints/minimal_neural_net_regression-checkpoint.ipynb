{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal neural network implementation (1/2)\n",
    "\n",
    "This is a \"bare bones\" or np-complete (numpy-complete) illustration of\n",
    "the neural networks for regression.\n",
    "\n",
    "To begin, let's look at linear functions, which we optimize in least squares linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "num_examples = len(x)\n",
    "w, b = (3,1)\n",
    "y = w*x + b\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hidden layer\n",
    "\n",
    "We'll add a single hidden layer with a number of \"neurons\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1\n",
    "H1 = 10\n",
    "X = np.array(x).reshape(num_examples,d)\n",
    "\n",
    "W0 = np.random.randn(1,H1)\n",
    "b0 = np.random.randn(1,H1)\n",
    "h1 = np.tanh(np.dot(X, W0) + b0)\n",
    "\n",
    "\n",
    "W1 = np.random.randn(H1, 1)\n",
    "b1 = np.random.randn(1,1)\n",
    "y = np.dot(h1,W1) + b1\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "_ = plt.plot(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two hidden layers\n",
    "\n",
    "Let's add another hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add a second hidden layer\n",
    "d=1\n",
    "H1 = 10\n",
    "H2 = 5\n",
    "\n",
    "W0 = np.random.randn(d, H1)\n",
    "b0 = np.random.randn(d, H1)\n",
    "h1 = np.tanh(np.dot(X, W0) + b0)\n",
    "\n",
    "W1 = np.random.randn(H1, H2)\n",
    "b1 = np.random.normal(1, H2)\n",
    "h2 = np.tanh(np.dot(h1,W1) + b1)\n",
    "\n",
    "\n",
    "W2 = np.random.randn(H2, 1)\n",
    "b2 = np.random.randn(1, 1)\n",
    "y = np.dot(h2, W2) + b2\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "_ = plt.plot(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "What happens if we replace the `np.tanh` function by the identity?\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "What happens if we replace the `np.tanh` function by the ReLU? By the sigmoid?\n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "Add a third hidden layer! Can you notice any change in the complexity (\"wigglyness\") of the random functions that are generated? What if you vary the number of neurons in each layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We'll now take a regression function and add noise. We'll then fit the function \n",
    "using gradient descent, where the gradients are computed using backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 200)\n",
    "num_examples = len(x)\n",
    "w, b = (3,1)\n",
    "m = x*np.cos(x)\n",
    "y = m + 0.2*np.random.normal(size=num_examples)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x,m,color='black',linewidth=2)\n",
    "plt.scatter(x,y,color='salmon',s=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "We'll use a rectified linear activation function, defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x*(x>0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters randomly\n",
    "W1 = np.random.randn(d,H1)\n",
    "b1 = np.random.randn(1,H1)\n",
    "W2 = np.random.randn(H1,1)\n",
    "b2 = np.random.randn(1,1)\n",
    "X = np.array(x).reshape(num_examples,d)\n",
    "Y = y.reshape(num_examples,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot the random initialization\n",
    "hidden_layer = relu(np.dot(X, W1) + b1) # note, ReLU activation\n",
    "Yhat = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x,m)\n",
    "plt.plot(x,Yhat)\n",
    "plt.scatter(x,Y,s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagate!\n",
    "\n",
    "Carry out gradient descent with backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize the parameters randomly\n",
    "W1 = np.random.randn(d,H1)\n",
    "b1 = np.random.randn(1,H1)\n",
    "W2 = np.random.randn(H1,1)\n",
    "b2 = np.random.randn(1,1)\n",
    "Y = y.reshape(num_examples,1)\n",
    "\n",
    "step_size = 1e-2\n",
    "steps = 200000\n",
    "loss = []\n",
    "for i in np.arange(steps):\n",
    "    \n",
    "    hidden_layer = relu(np.dot(X, W1) + b1) \n",
    "    Yhat = np.dot(hidden_layer, W2) + b2\n",
    "    current_loss = .5*np.mean((Y-Yhat)**2)\n",
    "    loss.append(current_loss)\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, current_loss))\n",
    "      \n",
    "    dloss = Yhat - Y\n",
    "    dloss /= num_examples\n",
    "\n",
    "    # backpropate the gradient to the parameters\n",
    "    # first backprop to parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, dloss)\n",
    "    db2 = np.sum(dloss)\n",
    "    \n",
    "    # next backprop to hidden layer\n",
    "    dhidden = np.dot(dloss, W2.T)\n",
    "    # backprop the ReLU non-linearity\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    \n",
    "    # finally backprop to W1,b1\n",
    "    dW1 = np.dot(X.T, dhidden)\n",
    "    db1 = np.sum(dhidden)\n",
    "    \n",
    "    # perform a parameter update\n",
    "    W1 += -step_size * dW1\n",
    "    b1 += -step_size * db1\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2\n",
    "    \n",
    "# Plot the loss as a function of gradient step\n",
    "# skip the first part of the algorithm where the decrease is very steep\n",
    "skip = 5000\n",
    "tail_loss = loss[skip:]\n",
    "plt.plot(np.arange(len(tail_loss))+skip, tail_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the fitted curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x,m)\n",
    "plt.plot(x,Yhat)\n",
    "plt.scatter(x,Y,s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "What happens if `relu` is replaced by the identity function?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
