{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetID: !! ADD NET_ID !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal neural network implementation\n",
    "\n",
    "This is a \"bare bones\" implementation of a 2-layer neural network for classification, using rectified linear units as activation functions. The code is from Andrej Karpathy; please see [this page](http://cs231n.github.io/neural-networks-case-study/) for an annotated description of the code.\n",
    "\n",
    "Your task in this part of the assigment is to extend this to a 3-layer network, and to experiment with some different settings of the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (a): Gradients (5 points)\n",
    "\n",
    "Calculate the gradients as described in the `assn7.pdf` document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['axes.facecolor'] = 'lightgray'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.3 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim([-1.5,1.5])\n",
    "plt.ylim([-1.5,1.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2_layer_network(H1=100):\n",
    "    # initialize parameters randomly\n",
    "    # H1 = 100 # size of hidden layer\n",
    "    W1 = np.random.randn(D,H1)\n",
    "    b1 = np.zeros((1,H1))\n",
    "    W2 = np.random.randn(H1,K)\n",
    "    b2 = np.zeros((1,K))\n",
    "\n",
    "    # some hyperparameters\n",
    "    step_size = 1e-1\n",
    "\n",
    "    # gradient descent loop\n",
    "    num_examples = X.shape[0]\n",
    "    for i in range(20000):\n",
    "  \n",
    "      # evaluate class scores, [N x K]\n",
    "      hidden_layer = np.maximum(0, np.dot(X, W1) + b1) # note, ReLU activation\n",
    "      scores = np.dot(hidden_layer, W2) + b2\n",
    "  \n",
    "      # compute the class probabilities\n",
    "      exp_scores = np.exp(scores)\n",
    "      probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "      # compute the loss: minus log prob\n",
    "      correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "      loss = np.sum(correct_logprobs)/num_examples\n",
    "      if i % 1000 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "  \n",
    "      # compute the gradient on scores\n",
    "      dscores = np.array(probs)\n",
    "      dscores[range(num_examples),y] -= 1\n",
    "      dscores /= num_examples\n",
    "  \n",
    "      # backpropate the gradient to the parameters\n",
    "      # first backprop into parameters W2 and b2\n",
    "      dW2 = np.dot(hidden_layer.T, dscores)\n",
    "      db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "      # next backprop into hidden layer\n",
    "      dhidden = np.dot(dscores, W2.T)\n",
    "      # backprop the ReLU non-linearity\n",
    "      dhidden[hidden_layer <= 0] = 0\n",
    "      # finally into W,b\n",
    "      dW1 = np.dot(X.T, dhidden)\n",
    "      db1 = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "      # perform a parameter update\n",
    "      W1 += -step_size * dW1\n",
    "      b1 += -step_size * db1\n",
    "      W2 += -step_size * dW2\n",
    "      b2 += -step_size * db2\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "W1, b1, W2, b2 = train_2_layer_network(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate training set accuracy\n",
    "hidden_layer = np.maximum(0, np.dot(X, W1) + b1)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the resulting classifier\n",
    "h = 0.015\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], W1) + b1), W2) + b2\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (b): Extend the code from two layers to three layers (15 points)\n",
    "\n",
    "Run the code provided in the notebook minimal neural network.ipynb and inspect it to be sure you understand how it works. (We did this in class!) Then, after working out the derivatives in part (a) above, extend the code by writing a function that implements a 3-layer version. Your function declaration should look like this:\n",
    "```python\n",
    "def train_3_layer_network(H1=100, H2=100)\n",
    "```    \n",
    "where H1 is the number of hidden units in the first layer, and H2 is the number of hidden units in the second layer. Then train a 3-layer network and display the classification results in your notebook, as is done for the 2-layer network in the starter code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (c): Experiment with different parameter settings (10 points)\n",
    "\n",
    "Now experiment with different network configurations and training parameters. For example, you can train models with different numbers of hidden nodes H1 and H2. Train at least three and no more than five networks. For each network, display the decision boundaries on the training data, and include a Markdown cell that describes its behavior relative to the other networks you train. Specifically, comment on how the different settings of the parameters change the bias and variance of the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
