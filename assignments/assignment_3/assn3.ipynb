{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "_Due: Midnight on Tuesday October 8, 2019_\n",
    "\n",
    "### NetID:\n",
    "\n",
    "In this assignment you will gain some experience with decision trees and random forests using two data sets. One is a diabetes data set, where the task is to predict the progression of the disease. The other a data set of real estate listings, where the task is to forecast the sale price of the house.\n",
    "\n",
    "#### Submission Instructions:\n",
    "\n",
    "Please fill out this _starter_ Jupyter Notebook, and submit __both__ this `.ipynb` file as well as a pdf file (via html).\n",
    "\n",
    " - In the notebook interface, choose `File -> Download as -> Notebook (ipynb)`.\n",
    " - In the notebook interface, choose `File -> Download as -> HTML`. Then open the html file, and print to pdf.\n",
    "\n",
    "Notes:\n",
    "\n",
    " - We are using the markdown cell-type for texts (and latex), and the code cell-type for the python code. Make sure you don't mix these up. You can change the type from the dropdown at the toolbar on the top.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Regression trees vs. random forests (20 pts)\n",
    "\n",
    "This problem is based on the `diabetes` dataset from the `sklearn` package. Please read about the dataset at [https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset). We will seek to predict the response, which is a quantitative measure of diabetes progression one year after baseline, using regression trees and random forests.\n",
    "\n",
    "The following cell imports the dataset as `diabetes` and names the predictor variables `diabetes_x` and the response `diabetes_y`. The names of the six predictor variables are also printed. For a more detailed description, use the `.DESCR` aspect of `diabetes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_x = diabetes.data\n",
    "diabetes_y = diabetes.target\n",
    "print(diabetes.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a): Building a Simple Regression Tree\n",
    "\n",
    "To start we will manually build a regression tree using only two of the predictor variables: `bmi` and `s5`. To keep things simple, build a tree that has exactly three nodes and four leaves. (i.e. the data is split into two parts initially and then each of those parts is again split one more time.) At each node you will need to evaluate each possible splitting point for both `bmi` and `s5` and pick the one that minimizes the RSS.\n",
    "\n",
    "When you have built the regression tree, create a scatter plot of `s5` versus `bmi`, color-coded by the response variable. In this plot, use vertical and horizontal lines to indicate the regions that your tree splits the data into. You may find the functions `plt.hlines()` and `plt.vlines()` to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi = diabetes_x[:,2]\n",
    "s5 = diabetes_x[:,8]\n",
    "rss_bmi = []\n",
    "rss_s5 = []\n",
    "\n",
    "# we put a wrapper on the np.mean function to avoid warnings from taking the average of an empty list\n",
    "def average(x):\n",
    "    if len(x) == 0:\n",
    "        return(0.0)\n",
    "    else:\n",
    "        return(np.mean(x))\n",
    "\n",
    "# the following starter code finds the best splits for bmi and bp at the root\n",
    "for i in range(len(bmi)):\n",
    "    left = np.where(bmi <= bmi[i])[0]\n",
    "    right = np.where(bmi > bmi[i])[0]\n",
    "    rss_bmi.append(np.sum((diabetes_y[left] - average(diabetes_y[left]))**2) + \n",
    "                   np.sum((diabetes_y[right] - average(diabetes_y[right]))**2))\n",
    "    left = np.where(s5 <= s5[i])[0]\n",
    "    right = np.where(s5 > s5[i])[0]\n",
    "    rss_s5.append(np.sum((diabetes_y[left] - average(diabetes_y[left]))**2) + \n",
    "                  np.sum((diabetes_y[right] - average(diabetes_y[right]))**2))\n",
    "    \n",
    "best_bmi_cut = np.argmin(rss_bmi)\n",
    "best_s5_cut = np.argmin(rss_s5)\n",
    "\n",
    "# You should feel free to rewrite the above code in any way that suits you.\n",
    "# Now complete the code to make the best split, and then split each child node, \n",
    "# and then visualize the tree, showing the four leaf rectangles in the space s5 vs. bmi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) Fitting a Full Regression Tree\n",
    "\n",
    "Now build a tree that uses all the predictor variables, has a more flexible structure, and is validated with a test set. Split the full dataset into a training set and a test set (50/50). Fit a regression tree to the training set using the function `DecisionTreeRegressor` from `sklearn.tree`. For now, use your best judgment to choose parameters for tree complexity; we will use analytical methods to choose parameters in later parts of this problem set. Some starter code is provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "regr = tree.DecisionTreeRegressor().fit() #tree parameters go inside the first set of parenthases and the training data\n",
    "                                          #goes in the second set of parenthases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) Plotting the Tree\n",
    "\n",
    "Plot your regression tree. To do so, we suggest that you use GraphViz in conjunction with `sklearn.tree.export_graphviz`. Once you install GraphViz, the following cell will plot the tree.\n",
    "\n",
    "Instructions for using GraphViz (Windows):\n",
    "\n",
    "1. Install GraphViz to your computer from the link [https://graphviz.gitlab.io/download/](https://graphviz.gitlab.io/download/).\n",
    "\n",
    "2. Install the Python package using `pip install graphviz` or `conda install graphviz`.\n",
    "\n",
    "3. Set a path to your computer's GraphViz installation (NOT the Python package). You can do so locally in this notebook by running something like `import os; os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'` (substituting in the location of your own GraphViz installation).\n",
    "\n",
    "4. You can now use the functions in the `graphviz` package with `sklearn.tree.export_graphviz`!\n",
    "\n",
    "Instructions for using GraphViz (Mac OS):\n",
    "\n",
    "1. Make sure you have the package manager Homebrew.\n",
    "\n",
    "2. Install GraphViz to your computer using `brew install graphviz`.\n",
    "\n",
    "3. Install the Python package using `pip install graphviz` or `conda install graphviz`.\n",
    "\n",
    "4. You can now use the functions in the `graphviz` package with `sklearn.tree.export_graphviz`! _Note: If you get an ExecutableNotFound error, you might have to set a path to your computer's GraphViz installation (NOT the Python package). You can do so locally in this notebook by running something like `import os; os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'` (substituting in the location of your own GraphViz installation)._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "#import os; os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n",
    "graphviz.Source(tree.export_graphviz(regr, out_file=None, feature_names=dat.feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) Evaluation\n",
    "\n",
    "Interpret your regression tree. What are some examples of variables that seem to correspond with higher or lower measures of diabetes progression? Find the MSE of the model using the test set. The `.predict` method for your model can help with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e) Random Forest\n",
    "\n",
    "Now use random forests to analyze the data with the `RandomForestRegressor` function from `sklearn.ensemble`. (Again, you may use your best judgment to choose the initial parameters for tree complexity.)\n",
    "\n",
    "__(i)__ What test MSE do you obtain, and how does it compare to the test MSE of the regression tree above? \n",
    "\n",
    "__(ii)__ According to the model, which variables are most important in predicting diabetes progression? (The `.feature_importances_` method of the model may help with this.)\n",
    "\n",
    "__(iii)__ Plot the MSE of the prediction against $m$, the number of variables considered at each split.\n",
    "\n",
    "__(iv)__ Comment on the plot you created and if it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "# Here is an example of how to use the random forest function in sklearn.ensemble.\n",
    "# The code below assumes that the training inputs and responses are loaded in the variables train_x and train_y\n",
    "# and that the test predictor variables are in test_x\n",
    "\n",
    "# dtr = ensemble.RandomForestRegressor(min_samples_leaf = 15, max_features = m)\n",
    "# regr = dtr.fit(train_x, train_y)\n",
    "# pred_y = regr.predict(test_x)\n",
    "# mse = sum(np.square(test_y-pred_y))\n",
    "\n",
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Analyzing Real Estate Data (40 pts)\n",
    "\n",
    "In this problem, you will train random forests on data from the website Zillow to forecast the sale price of real estate listings. Random forests are nonparametric methods for classification and regression. As discussed in class, the method is based on the following idea: a good predictor will have low bias and low variance. A deep decision tree has low bias, but high variance. To reduce the variance, multiple trees are fit and averaged together. By introducing randomness in the construction of the trees, the correlation between them is reduced, to facilitate the variance reduction.\n",
    "\n",
    "Read in the training and test sets as follows:\n",
    "\n",
    "`import pandas as pd\n",
    "train = pd.read_csv(\"zillow_train.csv\")\n",
    "test = pd.read_csv(\"zillow_test.csv\")`\n",
    "\n",
    "Use the following variables: `Lat`, `Long`, `ListPrice`, `SaleYear`, `Bathroom`, `Bedroom`, `BuildDecade`, `MajorRenov`, `FinishSqFt`, `LotSqFt`, `MSA`, `City`, `HighSchool`, `SalePrice`. You will build regression models to predict `SalePrice`.\n",
    "\n",
    "\n",
    "### (a) Explore the data\n",
    "\n",
    "Get an idea of what kind of data you're working with. As usual, you might ask yourself what $n$ (sample size) and $p$ (number of predictor variables) are here. Make plots of the distributions of the variables. Include a plot of the response, `SalePrice`. Does it appear that the data are \"raw\", or have they been pre-processed in different ways? If so, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Comment here on the above questions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b) Preliminary steps\n",
    "\n",
    "__(i)__ Some of the variables in the data are categorical; how many values do they take? (You may find the `.nunique` method of pandas to be useful here.) Why might factor variables with many categories present a problem when fitting decision trees? Describe a couple different ways of handling factor variables when fitting decision trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Your Markdown Here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(ii)__ Use your best judgement to modify the Zillow dataset to handle factor variables. In addition to `pandas` and `numpy`, it might be helpful to look at functions in `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(iii)__ We will soon use a few methods to predict `SalePrice`. Throughout, we will evaluate the predictions in terms of the absolute relative error:\n",
    "\n",
    "<center>$\\frac{1}{n}\\sum_{i=1}^n \\frac{\\mid Y_i - \\hat{Y}_i\\mid}{Y_i}$</center>\n",
    "\n",
    "Explain why this is a more appropriate choice of accuracy, compared with squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Your Markdown Here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c) Build models using random forests\n",
    "\n",
    "Build random forest models to predict `SalePrice` from the other variables, using the appropriate method from `sklearn.ensemble`. As in Question 1, one parameter to vary is `max_features`, or the number of variables allowed in each split; this regulates the correlation between the trees in the random forest by introducing randomness. Two more relevant parameters are `n_estimators` and `min_samples_leaf`, or number of trees and minimum node size, which regulate variance and bias.\n",
    "\n",
    "Train several random forest models, each time using different values of the parameters. Evaluate each model using 5-fold cross-validation (`sklearn.model_selection.KFold` may be a useful resource to perform k-fold cross-validation). For the sake of time, you may keep `n_estimators` low and constant. First vary `max_features` and create a plot of the cross-validation error versus the value of this parameter. Next vary `min_samples_leaf` a create a similar plot with the values of this parameter. \n",
    "\n",
    "Comment on how cross-validation error relates to `max_features` and `min_samples_leaf`, and how do you imagine it would relate to `n_estimators`? Does this make sense to you?\n",
    "\n",
    "Now find a combination of values for `max_features` and `min_samples_leaf` that approximately minimizes the cross-validation error. \n",
    "\n",
    "_Note: Use mean absolute error (`mae`) rather than mean squared error (`mse`) as the criterion for growing the trees. But then when you evaluate different models, compute the relative absolute error, as described above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starter Code to fill in and complete\n",
    "from sklearn import model_selection\n",
    "kf = model_selection.KFold(n_splits=5)\n",
    "\n",
    "x = \n",
    "y = \n",
    "\n",
    "for m in range( , ): # vary min_samples_leaf\n",
    "\n",
    "    dtr = ensemble.RandomForestRegressor(n_estimators = 15, min_samples_leaf = m, #these are the parameters to vary\n",
    "                                         max_features = 10, criterion = 'mae')\n",
    "    \n",
    "    for train_index, test_index in kf.split(x):\n",
    "        train_x = \n",
    "        train_y = \n",
    "        regr = dtr.fit(train_x, train_y)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d) Comparison to Least-Squares Regression\n",
    "\n",
    "Now build a least-squares linear model for the response variable as a function of the predictor variables using the training set. You may wish to use the `sklearn.linear_model.LinearRegression` function, described\n",
    "[here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "Experiment with different subsets of the predictor variables included in the linear model. \n",
    "\n",
    "Using the random forest model from Part (c) with the best combination of values for `max_features` and `min_samples_leaf` that you found, compare both the mean squared error and the relative absolute error on the test set from the random forest and linear models.\n",
    "\n",
    "Which model does a better job at prediction? Do you think the model with the higher MSE has higher variance or higher bias, or both?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Your Markdown Here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (e) Predicting SalePrice\n",
    "\n",
    "Read in the file \"zillow_part_e.csv\" which has 7000 houses with all the same variables as the training and testing set, except that the SalePrice variable is missing. \n",
    "\n",
    "Construct the best model you can on the training data. You can use random forests, or you may try to use gradient tree boosting, which is also available in sklearn.ensemble.  \n",
    "\n",
    "Using your best model, predict the sale prices for these 7000 houses. Students will be assigned extra credit according to which decile they are in for the predictive accuracy (relative absolute error). (The top 10% will receive 10 points extra credit, the next 10% 9 points, and so on.) \n",
    "\n",
    "Save your predictions in a file called \"zillow_predictions.csv\" and submit this file with your homework. Your csv file should only contain a single column of predictions, without a header, where the $i$-th row corresponds to the predicted sale price for the $i$-th row of the dataset read in from \"zillow_part_e.csv\", excluding the header.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "newdata = pd.read_csv(\"zillow_part_e.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
