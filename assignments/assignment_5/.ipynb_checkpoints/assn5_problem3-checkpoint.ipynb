{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Topic Models\n",
    "\n",
    "Due: Tuesday, November 5.\n",
    "\n",
    "This assignment has three problems. The first is about Bayesian inference. The second two are about topic models. You will first work with abstracts of scientific articles. These abstracts are obtained from arXiv.org, an open access repository for e-prints of articles in scientific fields maintained by Cornell University. You will then work with a collection of movie plots.\n",
    "\n",
    "*For your convenience, we have separated the problems into three notebooks: assn5_problem1.ipynb, assn5_problem2.ipynb, and assn5_problem3.ipynb. Submit your solutions in these three notebooks, printing out each as a separate pdf.*\n",
    "\n",
    "We provide significant \"starter code\" as discussed in lecture. We then ask you build topic models using the Python library gensim, and do some analysis over the topics obtained.\n",
    "\n",
    "We ask that you please at least start the assignment right away. If you have any difficulties running gensim we would like to know!\n",
    "\n",
    "\n",
    "# Problem 3: Topic Models on Movie Plots \n",
    "\n",
    "In this problem we will continue working with topic models, but this time with a new dataset. Instead of abstracts of scientific articles, we will create topic models over movie plot descriptions. This is a dataset containing descriptions of movies from Wikipedia. The dataset was [obtained](https://www.kaggle.com/jrobischon/wikipedia-movie-plots) from Kaggle, an online community of data scientists. We again provide extensive starter code to process the data.\n",
    "\n",
    "Spoiler alert! We will use the movie \"[Husbands and Wives](https://en.wikipedia.org/wiki/Husbands_and_Wives)\" as a running example...\n",
    "\n",
    "[<img width=200 src=\"https://upload.wikimedia.org/wikipedia/en/7/70/Husbands_moviep.jpg\">](https://en.wikipedia.org/wiki/Husbands_and_Wives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "logging.root.level = logging.CRITICAL \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# direct plots to appear within the cell, and set their style\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, the movie plot descriptions are in a CSV format in `movie_plots.csv`. The file is hosted on the Amazon Web Service s3. We'll use the `datascience` package to read this CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"https://s3.amazonaws.com/sds171/labs/lab07/movie_plots.csv\"\n",
    "data = pd.read_csv(filename)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the data a little more manageable, we restrict to movies that were released after 1980. We then pull out the titles and plots as lists, for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = data[data['Release Year'] > 1980]\n",
    "titles = list(movies['Title'])\n",
    "plots = list(movies['Plot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 2015\n",
    "print(\"Number of movies: %d\\n\" % movies.shape[0])\n",
    "print(\"Plot of \\\"%s\\\":\\n\" % titles[2015])\n",
    "print(plots[2015])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot description is from the movie \"[Husbands and Wives](https://en.wikipedia.org/wiki/Husbands_and_Wives)\"\n",
    "\n",
    "We don't have LaTeX markup in these documents, but we'll still use some regular expressions to do some simpe pre-processing of punctuation. There are lots of names in the plot descriptions, so we'll remove all the words that have a capitalized first letter. This will remove lots of non-name words as well, but this'll be sufficient for our goal of building a basic topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# replace '-' with ' ', then remove punctuation\n",
    "plots = [re.sub('-', ' ', plot) for plot in plots]\n",
    "plots = [re.sub('[^\\w\\s]', '', plot) for plot in plots]\n",
    "\n",
    "# remove tokens with a capitalized first letter \n",
    "# (broad stroke to remove names)\n",
    "plots = [re.sub('[A-Z]\\w*', '', plot) for plot in plots]\n",
    "# replace multiple spaces by a single space\n",
    "plots = [re.sub('[ ]+', ' ', plot) for plot in plots]\n",
    "\n",
    "print(plots[sample])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we further process each plot description by converting it to lower case, stripping leading and trailing white space, and then tokenizing by splitting on spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_tok = []\n",
    "for plot in plots:\n",
    "    processed = plot.lower().strip().split(' ')\n",
    "    plots_tok.append(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Further cleaning\n",
    "\n",
    "As in problem 2, we will remove tokens that have digits, possessives or contractions, or are empty strings.\n",
    "- `is_numeric(string)` checks if `string` has any numbers\n",
    "- `has_poss_contr(string)` checks if `string` has possessives or contractions\n",
    "- `empty_string(string)` checks if `string` is an empty string\n",
    "- `remove_string(string)` checcks if `string` should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(string):\n",
    "    return False\n",
    "\n",
    "def has_poss_contr(string):\n",
    "    return False\n",
    "\n",
    "def empty_string(string):\n",
    "    return False\n",
    "\n",
    "def remove_string(string):\n",
    "    return is_numeric(string) | has_poss_contr(string) | empty_string(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for plot in plots_tok:\n",
    "    filtered = []\n",
    "    for token in plot:\n",
    "        if not remove_string(token):\n",
    "            filtered.append(token)\n",
    "    temp.append(filtered)\n",
    "plots_tok = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that to build topic models, we require the following components:\n",
    "- A vocabulary of tokens that appear across all documents.\n",
    "- A mapping of those tokens to a unique integer identifier, because topic model algorithms treat words by these identifiers, and not the strings themselves. For example, we represent `'epidemic'` as `word2id['epidemic'] = 50`\n",
    "- The corpus, where each document in the corpus is a collection of tokens, where each token is represented by the identifier and the number of times it appears in the document. For example, in the first document above the token `'epidemic'`, which appears twice, is represented as `(50, 2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a vocabulary representing the tokens that have appeared across all the plot descriptions we have. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we can use the `Counter` class to build the vocabulary. The `Counter` is an extension of the Python dictionary, and also has key-value pairs. For the `Counter`, keys are the objects to be counted, while values are their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "for plot in plots_tok:\n",
    "    vocab.update(plot)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that removing rare words helps prevent our vocabulary from being too large. Many tokens appear only a few times across all the plot descriptions. Keeping them in the vocabulary increases subsequent computation time. Furthermore, their presence tends not to carry much significance for a document, since they can be considered as anomalies.\n",
    "\n",
    "We remove rare words by only keeping tokens that appear more than 25 times across all plot descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for token in vocab.elements():\n",
    "    if vocab[token] >= 50:\n",
    "        tokens.append(token)\n",
    "vocab = Counter(tokens)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that stop words are defined as very common words such as `'the'` and `'a'`. Removing stop words is important because their presence also does not carry much significance, since they appear in all kinds of texts.\n",
    "\n",
    "We will remove stop words by removing the 200 most common tokens across all the plot descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "for item in vocab.most_common(200):\n",
    "    stop_word = item[0]\n",
    "    stop_words.append(stop_word)\n",
    "tokens = []\n",
    "for token in vocab.elements():\n",
    "    if token not in stop_words:\n",
    "        tokens.append(token)\n",
    "vocab = Counter(tokens)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a mapping for tokens to unique identifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = vocab.items()\n",
    "id2word = {}\n",
    "word2id = {}\n",
    "idx = 0\n",
    "for word, count in vocab.items():\n",
    "    id2word[idx] = word\n",
    "    word2id[word] = idx\n",
    "    idx += 1\n",
    "    \n",
    "print(\"Number of tokens mapped: %d\" % len(id2word))\n",
    "print(\"Identifier for 'photograph': %d\" % word2id['photograph'])\n",
    "print(\"Word for identifier %d: %s\" % (word2id['photograph'], id2word[word2id['photograph']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will remove, for each plot description, the tokens that are not found in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for plot in plots_tok:\n",
    "    filtered = []\n",
    "    for token in plot:\n",
    "        if token in vocab:\n",
    "            filtered.append(token)\n",
    "    temp.append(filtered)\n",
    "plots_tok = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the corpus. Recall that the corpus should have the format\n",
    "```\n",
    "[(1841, 2), (2095, 2), (2096, 1), (2097, 1), (2098, 2), (105, 2), (2099, 1), (2100, 1), (270, 2), (1763, 1), (1870, 1), (2101, 1), (2017, 4), (633, 1), (1270, 1), (1093, 1), (2102, 1), (1197, 1), (113, 1), (1583, 1), (2103, 1), (2104, 2), (2105, 1), (873, 1), (1950, 1), (107, 1), (2106, 1), (2107, 1), (116, 1), (1436, 1), (62, 1), (2108, 1), (213, 1), (2109, 1), (1205, 1), (2110, 1), (1042, 1), (1275, 1), (1259, 1), (1342, 1), (2111, 1), (440, 1), (1662, 1), (374, 1), (663, 1)]\n",
    "```\n",
    "\n",
    "where each element is a pair containing the identifier for the token and the count of that token in just that plot description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for plot in plots_tok:\n",
    "    plot_count = Counter(plot)\n",
    "    corpus_doc = []\n",
    "    for item in plot_count.items():\n",
    "        pair = (word2id[item[0]], item[1])\n",
    "        corpus_doc.append(pair)\n",
    "    corpus.append(corpus_doc)\n",
    "\n",
    "print(\"Plot, tokenized:\\n\", plots_tok[sample], \"\\n\")\n",
    "print(\"Plot, in corpus format:\\n\", corpus[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to create our topic model!\n",
    "\n",
    "We again use gensim, a Python library to create topic models. Also, we again use the algorithm called latent dirichlet allocation implemented in the gensim library. \n",
    "\n",
    "**This step takes about 2 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=10, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics for Movies\n",
    "\n",
    "Your task is now to carry out the same steps as for problem 2 (arXiv abstracts), but now for this dataset of movie plots\n",
    "\n",
    "#### 3.2 Label the Topics\n",
    "Label all the 10 topics with your interpretation of what the topics are. \n",
    "\n",
    "#### 3.3 Table of Topics for Movies\n",
    "Create a function `create_movie_table(data, abstracts, corpus, lda_model)` which does the following:\n",
    "- Goes through every movie plot and finds the most likely topic for that plot.\n",
    "- Creates a table `movie_table` that has the following columns\n",
    "    - `title`: the title of the movie\n",
    "    - `topic`: the topic number of the most likely topic for each abstract\n",
    "    - `label`: the topic label of that topic number, which you assigned in part 1\n",
    "    - `prob`: the probability of that topic number\n",
    "    - `plot`: a string containing the first 200 characters of the plot\n",
    "- Show the first 10 rows of the table, then return the table\n",
    "\n",
    "#### 3.4 Analysis for selected movies\n",
    "Choose at least five movies, including 'Husbands and Wives' and discuss how the assignment of topics either does or does not make sense, according to your own understanding of the movies.\n",
    "Note that Wikipedia pages are given for most of the movies in the original data. For example, \n",
    "[https://en.wikipedia.org/wiki/Absence_of_Malice](https://en.wikipedia.org/wiki/Absence_of_Malice) is the page for \"Absence of Malice\"\n",
    "\n",
    "#### 3.5 Extra credit: Improve the model\n",
    "For extra credit, improve the topic model by improving the processing of the data and the vocabulary, and selecting a more appropriate number of topics. Describe how your new model gives an improvement over the \"quick and dirty\" topic model built above.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
